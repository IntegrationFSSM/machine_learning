\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{titlesec}

\geometry{hmargin=2.5cm,vmargin=2.5cm}

% Configuration des en-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\rhead{Projet Machine Learning & MLOps}
\lhead{Yassine Ennhili}
\cfoot{\thepage}

\begin{document}

\begin{titlepage}
    \centering
    {\Large \textbf{Université Cadi Ayyad}}\\
    {\Large \textbf{Faculté des Sciences Semlalia}}\\
    \vspace{1cm}
    % Placeholder pour logo si disponible
    % \includegraphics[width=0.3\textwidth]{logo_uca.png}
    \vspace{2cm}
    
    {\Huge \textbf{Rapport de Projet}}\\
    \vspace{0.5cm}
    {\Huge \textbf{Machine Learning \& MLOps}}\\
    \vspace{0.5cm}
    {\LARGE \textit{Prédiction de la survie des passagers du Titanic}}\\
    
    \vspace{3cm}
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
            \textbf{Réalisé par :}\\
            Yassine \textsc{Ennhili}
        \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
            \textbf{Encadré par :}\\
            Pr. Fahd \textsc{Kalloubi}
        \end{flushright}
    \end{minipage}
    
    \vspace{4cm}
    
    {\large Année Universitaire 2025-2026}
\end{titlepage}

\tableofcontents
\newpage

\section{Abstract}
Ce document présente une étude comparative de plusieurs algorithmes de Machine Learning (Régression Logistique, Random Forest, SVM) appliqués au jeu de données Titanic. L'objectif est de prédire la survie des passagers en mettant en œuvre un pipeline MLOps complet incluant le suivi des expériences avec MLflow.

Le code source et les données sont disponibles sur GitHub : \url{https://github.com/IntegrationFSSM/machine_learning}.

\section{Introduction Générale}
Le naufrage du Titanic le 15 avril 1912 est l'un des événements maritimes les plus tragiques et les plus étudiés de l'histoire moderne. Sur les 2224 passagers et membres d'équipage, 1502 ont péri, soit un taux de survie de seulement 32\%. Si le hasard a joué un rôle, il est avéré que certains groupes de personnes avaient plus de chances de survivre que d'autres (femmes, enfants, passagers de première classe).

Dans le cadre de ce projet universitaire, nous ne cherchons pas seulement à revisiter cette tragédie sous l'angle de l'histoire, mais à l'utiliser comme un cas d'étude canonique pour l'apprentissage automatique (Machine Learning). L'objectif est double :
\begin{enumerate}
    \item \textbf{Technique} : Construire un modèle prédictif performant capable de déterminer la survie d'un passager en fonction de ses caractéristiques.
    \item \textbf{Méthodologique} : Mettre en œuvre un pipeline \textbf{MLOps} (Machine Learning Operations) complet, intégrant les meilleures pratiques de l'industrie : versionning du code (Git), reproductibilité des expériences (MLflow) et rédaction scientifique (LaTeX).
\end{enumerate}

Ce rapport s'articule autour de quatre axes majeurs : une analyse exploratoire approfondie des données, la présentation théorique des algorithmes retenus, la méthodologie expérimentale et enfin l'analyse critique des résultats obtenus.

\section{État de l'Art et Cadre Théorique}

\subsection{Apprentissage Supervisé et Classification}
Le problème posé est un problème de \textbf{classification binaire supervisée}.
Soit un ensemble de données d'apprentissage $D = \{(x_i, y_i)\}_{i=1}^N$ où $x_i \in \mathbb{R}^d$ représente le vecteur des caractéristiques (features) du $i$-ème passager et $y_i \in \{0, 1\}$ son étiquette (label), avec 1 pour "Survivant" et 0 pour "Décédé".
L'objectif est d'apprendre une fonction $f: \mathbb{R}^d \rightarrow \{0, 1\}$ qui minimise une fonction de coût (Loss Function) sur des données non vues.

\subsection{Algorithmes Étudiés}

\subsubsection{Régression Logistique}
Bien que nommée "régression", il s'agit d'un classifieur linéaire. Elle modélise la probabilité d'appartenance à la classe positive $P(Y=1|X)$ à l'aide de la fonction sigmoïde (ou logistique) $\sigma(z) = \frac{1}{1 + e^{-z}}$.
Le modèle s'écrit :
\begin{equation}
P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + \exp(-(w_1 x_1 + \dots + w_d x_d + b))}
\end{equation}
Les paramètres $w$ et $b$ sont estimés en maximisant la \textbf{vraisemblance} (Log-Likelihood) ou, de manière équivalente, en minimisant la \textbf{Log-Loss} (Entropie Croisée binaire).
Ce modèle est particulièrement apprécié pour son \textbf{interprétabilité} : le signe et la magnitude des coefficients $w_j$ indiquent directement l'influence de la variable $j$ sur la probabilité de survie.

\subsubsection{Support Vector Machine (SVM)}
Les Machines à Vecteurs de Support cherchent à trouver l'hyperplan séparateur qui maximise la \textbf{marge} entre les deux classes.
Pour des données linéairement séparables, l'hyperplan est défini par $w^T x + b = 0$. La marge est donnée par $\frac{2}{||w||}$. Maximiser la marge revient à minimiser $\frac{1}{2}||w||^2$ sous contraintes.

Dans le cas non-linéaire (fréquent pour le Titanic), on utilise "l'astuce du noyau" (\textbf{Kernel Trick}). On projette implicitement les données dans un espace de dimension supérieure $\mathcal{H}$ via une fonction $\phi(x)$, tel que le produit scalaire soit calculé par une fonction noyau $K(x, x')$.
Nous avons utilisé le noyau RBF (Radial Basis Function) :
\begin{equation}
K(x, x') = \exp(-\gamma ||x - x'||^2)
\end{equation}
Le paramètre $\gamma$ contrôle la portée de l'influence d'un seul exemple d'apprentissage, et $C$ (paramètre de régularisation) contrôle le compromis entre maximisation de la marge et minimisation des erreurs de classification.

\subsubsection{Random Forest (Forêts Aléatoires)}
Le Random Forest est une méthode \textbf{ensembliste} de type Bagging (Bootstrap Aggregating). L'idée est de combiner plusieurs classifieurs "faibles" (ici, des arbres de décision) pour créer un classifieur robuste.
\begin{itemize}
    \item \textbf{Bootstrap} : Chaque arbre est entraîné sur un échantillon tiré avec remise dans le jeu de données original.
    \item \textbf{Random Feature Selection} : À chaque nœud de l'arbre, seule une sous-partie aléatoire des features est considérée pour la scission.
\end{itemize}
La prédiction finale est obtenue par \textbf{vote majoritaire} des arbres. Cette méthode réduit considérablement la variance (et donc le surapprentissage) par rapport à un arbre de décision unique. De plus, elle permet de calculer l'\textbf{importance des variables} (Feature Importance) en mesurant la diminution moyenne de l'impureté de Gini apportée par chaque variable.

\section{Données et Prétraitement (Data Engineering)}

\subsection{Description Détaillée du Dataset}
Le jeu de données "Titanic" contient 1309 entrées (Train + Test combinés généralement) et 12 variables :
\begin{itemize}
    \item \textbf{PassengerId} : Identifiant unique (non informatif).
    \item \textbf{Survived} : Cible (0/1).
    \item \textbf{Pclass} : Classe du billet (1 = Haute, 2 = Moyenne, 3 = Basse). Proxy du statut socio-économique.
    \item \textbf{Name} : Nom complet. Contient des titres (Mr, Mrs, Miss, Master) qui peuvent être extraits (Feature Engineering avancé).
    \item \textbf{Sex} : Sexe (male/female). Facteur déterminant historique.
    \item \textbf{Age} : Âge en années. Donnée critique mais comportant environ 20\% de valeurs manquantes.
    \item \textbf{SibSp} : Nombre de frères/soeurs/époux à bord.
    \item \textbf{Parch} : Nombre de parents/enfants à bord.
    \item \textbf{Ticket} : Numéro du ticket (souvent alphanumérique complexe).
    \item \textbf{Fare} : Tarif passager. Fortement corrélé à la Pclass.
    \item \textbf{Cabin} : Numéro de cabine. Comporte énormément de valeurs manquantes (>70\%).
    \item \textbf{Embarked} : Port d'embarquement (C = Cherbourg, Q = Queenstown, S = Southampton).
\end{itemize}

\subsection{Stratégie de Nettoyage et d'Imputation}
La qualité des données est primordiale ("Garbage In, Garbage Out").
\subsubsection{Traitement des Valeurs Manquantes}
Pour la variable \textbf{Age}, la distribution est proche d'une gaussienne mais avec une queue à droite. Remplacer par la moyenne (29.7 ans) est une approche naïve. Nous avons préféré la \textbf{médiane} qui est plus robuste aux valeurs extrêmes (outliers).
\textit{Note : Une approche plus fine consisterait à imputer l'âge en fonction du Titre (Master a un âge moyen plus bas que Mr).}

Pour \textbf{Embarked}, seules 2 valeurs manquaient. L'imputation par le \textbf{mode} (valeur la plus fréquente, ici 'S') est la norme statistique standard pour les variables catégorielles.

La variable \textbf{Cabin} a été supprimée car le taux de valeurs manquantes est trop critique pour tenter une imputation fiable sans introduire un biais massif.

\subsection{Feature Engineering et Encodage}
\subsubsection{Encodage des Variables Catégorielles}
Les modèles mathématiques ne manipulent que des chiffres.
\begin{itemize}
    \item Pour \textbf{Sex} (binaire), un simple mapping (male=0, female=1) ou One-Hot Encoding donne le même résultat informationnel sur un vecteur de dimension 1.
    \item Pour \textbf{Embarked} (C, Q, S), utiliser un Label Encoding (0, 1, 2) introduirait une relation d'ordre arbitraire ($2 > 1 > 0$) qui n'existe pas géographiquement. Nous avons donc utilisé le \textbf{One-Hot Encoding}, créant 3 variables binaires : $Is\_S$, $Is\_C$, $Is\_Q$.
\end{itemize}

\subsubsection{Mise à l'Échelle (Normalization)}
La variable \textbf{Fare} peut varier de 0 à 500+, tandis que \textbf{Age} varie de 0 à 80. Sans normalisation, le modèle SVM (basé sur des distances euclidiennes) serait totalement dominé par la variable Fare.
Nous avons appliqué le \textbf{StandardScaler} :
\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}
où $\mu$ est la moyenne et $\sigma$ l'écart-type. Cela ramène toutes les variables à une distribution centrée réduite (moyenne 0, variance 1), assurant une convergence plus rapide et équitable des algorithmes de gradient.

\section{Protocole MLOps et Outils}

\subsection{Environnement de Développement}
Le projet a été développé en \textbf{Python}, langage standard de la Data Science, en utilisant les librairies \texttt{scikit-learn} pour la modélisation et \texttt{pandas} pour la manipulation de données.

\subsection{Suivi des Expériences avec MLflow}
Dans une démarche scientifique, il est crucial de pouvoir reproduire une expérience. Nous avons intégré \textbf{MLflow Tracking} pour enregistrer systématiquement à chaque exécution (run) :
\begin{itemize}
    \item Les \textbf{Hyperparamètres} : Profondeur des arbres, coefficient de régularisation C, gamma du noyau RBF.
    \item Les \textbf{Métriques} : Accuracy, Precision, Recall, F1-Score.
    \item Les \textbf{Artefacts} : Le modèle sérialisé (format pickle/MLmodel) et les figures générées (matrices de confusion).
\end{itemize}
L'interface utilisateur (UI) de MLflow permet ensuite de comparer visuellement les performances des différents runs et de sélectionner le meilleur modèle pour le déploiement.

\subsection{Contrôle de Version (Git)}
L'intégralité du code est versionnée sous Git et hébergée sur GitHub. Cela assure la traçabilité des modifications, la collaboration et la sauvegarde sécurisée du patrimoine intellectuel du projet.

\section{Résultats et Discussion}

\subsection{Métriques d'Évaluation}
Pour évaluer nos modèles de classification, nous utilisons plusieurs métriques complémentaires :
\begin{itemize}
    \item \textbf{Accuracy} (Précision globale) : Proportion de prédictions correctes.
    $$ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} $$
    \item \textbf{Precision} : Parmi les passagers prédits survivants, combien le sont réellement ?
    $$ Precision = \frac{TP}{TP + FP} $$
    \item \textbf{Recall (Rappel)} : Parmi les vrais survivants, combien en avons-nous détecté ?
    $$ Recall = \frac{TP}{TP + FN} $$
    \item \textbf{F1-Score} : Moyenne harmonique entre Precision et Recall, utile pour les classes déséquilibrées.
    $$ F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall} $$
\end{itemize}

\subsection{Analyse Comparative}
Voici les performances obtenues sur l'ensemble de test (20\% des données) :

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{images/model_comparison.png}
    \caption{Comparaison des performances (Accuracy)}
    \label{fig:metrics}
\end{figure}

\subsubsection{Analyse du Random Forest}
Le Random Forest obtient généralement les meilleurs résultats (~80-82\%). Cela s'explique par la nature non-linéaire des frontières de décision. Par exemple, la règle "Femmes et Enfants d'abord" est une interaction complexe entre Age et Sex que les arbres de décision capturent naturellement.
La \textbf{Feature Importance} confirme cette hypothèse :
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/feature_importance_Random_Forest.png}
    \caption{Importance des variables (Random Forest)}
\end{figure}
Les variables \texttt{Sex\_female}, \texttt{Age} et \texttt{Fare} dominent largement le modèle.

\subsubsection{Analyse des Erreurs (Matrices de Confusion)}
Les matrices nous montrent où les modèles se trompent.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/confusion_matrix_Random_Forest.png}
    \caption{Matrice de Confusion du meilleur modèle}
\end{figure}
On observe souvent un nombre non négligeable de Faux Négatifs (Survivants prédits morts). Cela peut être dû à des cas limites (hommes adultes ayant survécu) que le modèle, basé sur les statistiques globales, a du mal à identifier sans  features plus spécifiques (comme la position de la cabine ou le statut militaire).

\section{Conclusion et Perspectives}

Ce projet a permis de construire une chaîne de traitement complète allant de la donnée brute à un modèle prédictif déployable.
Nous avons montré que :
\begin{enumerate}
    \item Le prétraitement (imputation, scaling) est une étape critique qui conditionne la performance.
    \item Les méthodes ensemblistes (Random Forest) surpassent souvent les modèles linéaires simples sur ce type de données tabulaires hétérogènes.
    \item L'approche MLOps avec MLflow garantit une rigueur scientifique indispensable.
\end{enumerate}

\textbf{Perspectives d'amélioration} :
Pour aller plus loin et viser une accuracy > 85\%, nous pourrions :
\begin{itemize}
    \item \textbf{Feature Engineering avancé} : Extraire le Titre des noms (Mr, Dr, Rev), créer une variable "Taille de famille" (SibSp + Parch + 1), ou discretiser l'Âge et le Tarif.
    \item \textbf{Optimisation des hyperparamètres} : Utiliser \texttt{GridSearchCV} ou \texttt{Optuna} pour affiner les paramètres du Random Forest (profondeur max, nombre d'estimateurs).
    \item \textbf{Stacking} : Combiner les prédictions du SVM et du Random Forest via un méta-modèle.
\end{itemize}

\newpage
\listoffigures
\newpage
\tableofcontents

\end{document}
